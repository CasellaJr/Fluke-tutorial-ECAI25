{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing client-server communication with `fluke`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through the steps required to handle the communication between clients and server when implementing a new federated learning algorithm using the `fluke` library.\n",
    "\n",
    "```{attention}\n",
    "This tutorial does not go into the details of the implementation, but it provides a quick overview of the steps required to implement a new federated learning algorithm.\n",
    "```\n",
    "\n",
    "Try this notebook: [![Open in Colab](https://img.shields.io/badge/Open_in_Colab-blue?style=flat-square&logo=google-colab&logoColor=yellow&labelColor=gray)\n",
    "](https://colab.research.google.com/github/CasellaJr/Fluke-tutorial-ECAI25/blob/main/3_fluke_communication.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LG-FedAVG example\n",
    "\n",
    "We have seen in the theoretical part of the tutorial that in LG-FedAVG, clients send only the global model updates to the server, while keeping their local model private. To implement this behavior in `fluke`, we need to customize the `Client` and `Server` classes to handle the communication accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, we need to define a model that separates the global and local parts.\n",
    "`fluke` provides a convenient `EncoderHeadNet` class that allows us to easily create such models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fluke.nets import EncoderHeadNet\n",
    "\n",
    "# The encoder/backbone\n",
    "class MLP_E(Module):\n",
    "\n",
    "    def __init__(self, hidden_size: tuple[int, int] = (200, 100)):\n",
    "        super(MLP_E, self).__init__()\n",
    "        self.fc1 = Linear(28 * 28, hidden_size[0])\n",
    "        self.fc2 = Linear(hidden_size[0], hidden_size[1])\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# The head/classifier\n",
    "class MLP_D(Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int = 100):\n",
    "        super(MLP_D, self).__init__()\n",
    "        self.fc3 = Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.fc3(x)\n",
    "\n",
    "# The complete model\n",
    "class MLP(EncoderHeadNet):\n",
    "    def __init__(self, hidden_size: tuple[int, int] = (200, 100)):\n",
    "        super(MLP, self).__init__(\n",
    "            MLP_E(hidden_size), MLP_D(hidden_size[1])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the client that only needs to share the head instead of the full model.\n",
    "To do this we override the `send_model` and receive_model methods to only send/receive the global part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from torch.nn.modules import Module\n",
    "\n",
    "from fluke.algorithms import PersonalizedFL  # NOQA\n",
    "from fluke.client import Client  # NOQA\n",
    "from fluke.comm import Message  # NOQA\n",
    "from fluke.config import OptimizerConfigurator  # NOQA\n",
    "from fluke.data import FastDataLoader  # NOQA\n",
    "from fluke.nets import EncoderHeadNet, HeadGlobalEncoderLocalNet  # NOQA\n",
    "from fluke.server import Server  # NOQA\n",
    "from fluke.utils.model import safe_load_state_dict  # NOQA\n",
    "\n",
    "\n",
    "class MyLGFedAVGClient(Client):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        index: int,\n",
    "        model: EncoderHeadNet,\n",
    "        train_set: FastDataLoader,\n",
    "        test_set: FastDataLoader,\n",
    "        optimizer_cfg: OptimizerConfigurator,\n",
    "        loss_fn: Module,\n",
    "        local_epochs: int = 3,\n",
    "        fine_tuning_epochs: int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            index=index,\n",
    "            train_set=train_set,\n",
    "            test_set=test_set,\n",
    "            optimizer_cfg=optimizer_cfg,\n",
    "            loss_fn=loss_fn,\n",
    "            local_epochs=local_epochs,\n",
    "            fine_tuning_epochs=fine_tuning_epochs,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.model = HeadGlobalEncoderLocalNet(model)\n",
    "        self._save_to_cache()\n",
    "\n",
    "    # The client communicates with the server through the channel\n",
    "    # The channel is a field of the Client class and it is a shared (with the server) reference\n",
    "    def send_model(self) -> None:\n",
    "        # The communication happens through Messages\n",
    "        # Each message has a type (here \"model\") and a payload (here the model)\n",
    "        # + the sender (the client ID in this case) and the receiver (the server)\n",
    "        # `inmemory=True` means that the message is kept in memory and not serialized to disk\n",
    "        message_model = Message(self.model.get_global(), \"model\", self.index, inmemory=True)\n",
    "        self.channel.send(\n",
    "            message=message_model,\n",
    "            mbox=\"server\"\n",
    "        )\n",
    "\n",
    "    def receive_model(self) -> None:\n",
    "        # The client receives only the global model from the server\n",
    "        msg = self.channel.receive(self.index, \"server\", msg_type=\"model\")\n",
    "        # Update only the global part of the model\n",
    "        safe_load_state_dict(self.model.get_global(), msg.payload.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server customization is not strictly necessary in case we assume that the configuration will be done \n",
    "coherently (i.e., the server does not have a test set). Otherwise, we can force it by changing the server constructor which will ignore any test set passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLGFedAVGServer(Server):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "        test_set: FastDataLoader,  # not used\n",
    "        clients: Sequence[Client],\n",
    "        weighted: bool = False,\n",
    "    ):\n",
    "        super().__init__(model=model, test_set=None, clients=clients, weighted=weighted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm now just need to define the `get_client_class` and `get_server_class` methods to use the customized classes.\n",
    "Note that it inherits from `PersonalizedFL` since LG-FedAVG is a personalized federated learning algorithm.\n",
    "The main difference with a standard `CentralizedFL` is that the client must define a model type rather than receiving it from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLGFedAVG(PersonalizedFL):\n",
    "\n",
    "    def get_client_class(self) -> type[Client]:\n",
    "        return MyLGFedAVGClient\n",
    "\n",
    "    def get_server_class(self) -> type[Server]:\n",
    "        return MyLGFedAVGServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.data import DataSplitter\n",
    "from fluke.data.datasets import Datasets\n",
    "from fluke import DDict\n",
    "from fluke.utils.log import Log\n",
    "from fluke.evaluation import ClassificationEval\n",
    "from fluke import FlukeENV\n",
    "\n",
    "env = FlukeENV()\n",
    "env.set_seed(42) # we set a seed for reproducibility\n",
    "env.set_device(\"cpu\")\n",
    "\n",
    "dataset = Datasets.get(\"mnist\", path=\"./data\")\n",
    "env.set_evaluator(ClassificationEval(eval_every=1, n_classes=dataset.num_classes))\n",
    "\n",
    "# We split the data client-side in train and test (80%-20%) - no server test set\n",
    "splitter = DataSplitter(dataset=dataset, distribution=\"iid\", client_split=0.2, server_test=False)\n",
    "\n",
    "# Withouth a test on the server, we need to set post_fit evaluation client-side\n",
    "env.set_eval_cfg(post_fit=True)\n",
    "\n",
    "client_hp = DDict(\n",
    "    model=MLP(),\n",
    "    batch_size=10,\n",
    "    local_epochs=5,\n",
    "    loss=\"CrossEntropyLoss\",\n",
    "    optimizer=DDict(lr=0.1),\n",
    "    scheduler=DDict(\n",
    "      gamma=1,\n",
    "      step_size=1)\n",
    ")\n",
    "\n",
    "hyperparams = DDict(client=client_hp,\n",
    "                    server=DDict(weighted=True),\n",
    "                    model=MLP_D()) # Here the shared model is only the head!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluke.algorithms.lg_fedavg import LGFedAVG\n",
    "algorithm = LGFedAVG(n_clients=100,\n",
    "                       data_splitter=splitter,\n",
    "                       hyper_params=hyperparams)\n",
    "\n",
    "logger = Log()\n",
    "algorithm.set_callbacks(logger)\n",
    "logger.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm.run(n_rounds=5, eligible_perc=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluke-tutorial (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
